{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "\n",
    "from lora_model import LoraModelForCasualLM\n",
    "from utils.common import download_from_driver\n",
    "from prepare_data import create_datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 gpu_id: int,\n",
    "                 is_ddp_training: bool = True,\n",
    "                 output_dir: str = 'checkpoints/',\n",
    "                 num_epochs: int = 10,\n",
    "                 max_length: int = 128,\n",
    "                 batch_size: int = 8,\n",
    "                 mixed_precision_dtype=None,\n",
    "                 gradient_accumulation_steps: int = 16):\n",
    "        \"\"\"\n",
    "        Initialize the Trainer class.\n",
    "\n",
    "        Args:\n",
    "            model: Pretrained model object.\n",
    "            tokenizer: Tokenizer object for text processing.\n",
    "            num_epochs: Number of training epochs.\n",
    "            max_length: Maximum sequence length.\n",
    "            batch_size: Training batch size.\n",
    "            gpu_id: GPU ID for training.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_ddp_training = is_ddp_training\n",
    "\n",
    "        self.gpu_id = gpu_id\n",
    "        self.model = model.to(f\"cuda:{self.gpu_id}\")\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "\n",
    "        self.mixed_precision_dtype = mixed_precision_dtype\n",
    "        self.ctx = None\n",
    "        self.gradscaler = None\n",
    "\n",
    "        # set mixed precision context\n",
    "        self.set_mixed_precision_context(mixed_precision_dtype)\n",
    "\n",
    "    def set_mixed_precision_context(self, mixed_precision_dtype):\n",
    "        \n",
    "        # TODO: Setup mixed precision training context\n",
    "\n",
    "        if mixed_precision_dtype is None:\n",
    "            \n",
    "            # If 'mixed_precision_dtype' is None, use 'nullcontext',\n",
    "            self.ctx = nullcontext()\n",
    "\n",
    "        else:\n",
    "        \n",
    "            # TODO Otherwise, use 'torch.amp.autocast' context with the specified dtype, and initialize GradScaler if mixed_precision_dtype is float16.\n",
    "            self.ctx = torch.cuda.amp.autocast(dtype = mixed_precision_dtype)\n",
    "            self.gradscaler = torch.cuda.amp.GradScaler() if mixed_precision_dtype == torch.float16 else None\n",
    "\n",
    "    def _set_ddp_training(self):\n",
    "\n",
    "        # TODO: Initialize the DistributedDataParallel wrapper for the model.\n",
    "        # You would need to pass the model and specify the device IDs\n",
    "        # and output device for the data parallelism.\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids = [self.gpu_id], output_device = self.gpu_id)\n",
    "        \n",
    "    def _run_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Run a single training batch.\n",
    "\n",
    "        Args:\n",
    "            batch: Batch data.\n",
    "\n",
    "        Returns:\n",
    "            Loss value for the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        with self.ctx:\n",
    "            outputs = self.model(**batch)\n",
    "            loss = outputs.loss / self.gradient_accumulation_steps  # Normalize loss\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        # TODO: If 'mixed_precision_dtype' is torch.float16, you have to modify the backward using the gradscaler.\n",
    "        if self.mixed_precision_dtype == torch.float16:\n",
    "\n",
    "            ### YOUR CODE HERE ###\n",
    "            self.gradscaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "    def _run_epoch(self, train_dataloader, epoch):\n",
    "        \"\"\"\n",
    "        Run a single training epoch.\n",
    "\n",
    "        Args:\n",
    "            train_loader: Training data loader.\n",
    "            epoch: Current epoch number.\n",
    "\n",
    "        Returns:\n",
    "            Total loss value for the epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        epoch_loss = 0\n",
    "        self.model.train()\n",
    "\n",
    "        if _is_master_process():\n",
    "            train_progress_bar = tqdm(\n",
    "                train_dataloader, desc=f\"Epoch {epoch + 1} [Training]\", position=0, leave=False)\n",
    "        else:\n",
    "            train_progress_bar = train_dataloader\n",
    "\n",
    "        # Add counter for gradient accumulation\n",
    "        steps = 0\n",
    "        self.optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n",
    "        for step, batch in enumerate(train_progress_bar):\n",
    "            steps += 1\n",
    "            batch = {key: value.to(self.gpu_id)\n",
    "                     for key, value in batch.items()}\n",
    "            loss = self._run_batch(batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Perform optimizer step and reset gradients after accumulating enough gradients\n",
    "            if steps % self.gradient_accumulation_steps == 0:\n",
    "\n",
    "                # If 'mixed_precision_dtype' is torch.float16, you have to modify the gradient update step using the gradscaler.\n",
    "                if self.mixed_precision_dtype == torch.float16:\n",
    "\n",
    "                    ### YOUR CODE HERE ###\n",
    "                    \n",
    "                    # TODO: optimizer step\n",
    "\n",
    "                    # TODO: update scaler factor\n",
    "                    self.gradscaler.step(self.optimizer)\n",
    "                    self.gradscaler.update()\n",
    "                    pass\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "        epoch_loss /= (len(train_dataloader) /\n",
    "                       self.gradient_accumulation_steps)\n",
    "        return epoch_loss\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        path_dir = f\"{self.output_dir}/epoch_{epoch}\"\n",
    "\n",
    "        # check path_dir exited\n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "\n",
    "        # save checkpoints\n",
    "        if self.is_ddp_training and _is_master_process():\n",
    "            self.model.module.save_pretrained(f'epoch_{epoch}_checkpoint')\n",
    "        else:\n",
    "            self.model.save_pretrained(f'epoch_{epoch}_checkpoint')\n",
    "\n",
    "        print(\"Done saved at\", f'epoch_{epoch}_checkpoint')\n",
    "\n",
    "    def prepare_dataloader(self, train_dataset, eval_dataset):\n",
    "\n",
    "        # TODO: Prepare the training DataLoader. Initialize 'DataLoader' with 'train_dataset'\n",
    "        # and the appropriate 'batch_size'.\n",
    "        # Depending on whether the training is distributed (is_ddp_training),\n",
    "        # use 'DistributedSampler' for 'sampler' argument, else use 'None'.\n",
    "        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\"\n",
    "        # Also add drop_last to True.\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        data_trainloader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                       batch_size = self.batch_size,\n",
    "                                                       sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if self.is_ddp_training else None,\n",
    "                                                       collate_fn = transformers.DataCollatorForSeq2Seq(tokenizer = self.tokenizer, padding = True,\n",
    "                                                                                                        return_tensors = \"pt\"))\n",
    "                                                    \n",
    "\n",
    "        # TODO: Prepare the evaluation DataLoader. Initialize 'DataLoader' with 'eval_dataset',\n",
    "        # the appropriate 'batch_size', and 'SequentialSampler' for 'sampler'.\n",
    "        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\".\n",
    "        # Also add drop_last to True.\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        data_testloader = torch.utils.data.DataLoader(dataset = eval_dataset,\n",
    "                                                      batch_size = self.batch_size,\n",
    "                                                      sampler = torch.utils.data.SequentialSampler(eval_dataset),\n",
    "                                                      collate_fn = transformers.DataCollatorForSeq2Seq(tokenizer = self.tokenizer,\n",
    "                                                                                                        padding = True, \n",
    "                                                                                                       return_tensors = \"pt\"))\n",
    "                                                                        \n",
    "\n",
    "        return data_trainloader, data_testloader\n",
    "\n",
    "    def _eval(self, eval_dataloader, epoch: int):\n",
    "        avg_loss = 0\n",
    "        model.eval()\n",
    "        if _is_master_process():\n",
    "            eval_progress_bar = tqdm(\n",
    "                eval_dataloader, desc=f\"Epoch {epoch + 1} [Evaluation]\", position=0, leave=False)\n",
    "        else:\n",
    "            eval_progress_bar = eval_dataloader\n",
    "\n",
    "\n",
    "        for batch in eval_progress_bar:\n",
    "            with self.ctx:\n",
    "                with torch.no_grad():\n",
    "                    if not self.is_ddp_training:\n",
    "                        outputs = self.model(**batch.to(self.gpu_id))\n",
    "                    else:\n",
    "                        outputs = self.model(**batch)\n",
    "            avg_loss += outputs.loss.item()\n",
    "        avg_loss = avg_loss/(len(eval_dataloader))\n",
    "        return avg_loss\n",
    "\n",
    "    def run(self, data_path: str, size_valid_set: int = 0.25, seed: int = 123):\n",
    "        \"\"\"\n",
    "        Run the training process.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        train_dataset, eval_dataset = create_datasets(\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "            data_path=data_path,\n",
    "            size_valid_set=size_valid_set,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        train_dataloader, eval_dataloader = self.prepare_dataloader(\n",
    "            train_dataset, eval_dataset)\n",
    "\n",
    "        if self.is_ddp_training:\n",
    "            self._set_ddp_training()\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            if self.is_ddp_training:\n",
    "                train_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_loss = self._run_epoch(train_dataloader, epoch)\n",
    "            if self.is_ddp_training:\n",
    "                dist.barrier() \n",
    "            if _is_master_process() or (epoch == self.num_epochs - 1):\n",
    "                eval_loss = self._eval(\n",
    "                    eval_dataloader=eval_dataloader, epoch=epoch)\n",
    "\n",
    "                print(\n",
    "                    f\"epoch = {epoch+1} | avg_train_loss = {train_loss} | eval_loss = {eval_loss}\")\n",
    "            \n",
    "            if _is_master_process():\n",
    "                self._save_checkpoint(epoch=epoch+1)\n",
    "\n",
    "def load_tokenizer_from_pretrained_model(model_path):\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    architecture = config.architectures[0]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, trust_remote_code=True, device_map={\"\": torch.device(f\"cuda:{0}\")})\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if _is_master_process():\n",
    "        print('Completed to load config & tokenizer')\n",
    "\n",
    "    if \"Llama\" in architecture:\n",
    "        if _is_master_process():\n",
    "            print(\"Setting EOS, BOS, UNK, and PAD tokens for LLama tokenizer\")\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": \"</s>\",\n",
    "                \"bos_token\": \"</s>\",\n",
    "                \"unk_token\": \"</s>\",\n",
    "            }\n",
    "        )\n",
    "        tokenizer.pad_token_id = (\n",
    "            0  # unk. we want this to be different from the eos token\n",
    "        )\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def _is_master_process():\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    return ddp_rank == 0\n",
    "\n",
    "\n",
    "def load_pretrained_model(local_rank, model_path: str = \"\"):\n",
    "    # TODO: Load a pretrained AutoModelForCausalLM from the 'model_path'.\n",
    "    # Make sure to set 'device_map' to '{\"\": torch.device(f\"cuda:{local_rank}\")}' for DDP training\n",
    "    # and trust_remote_code=True.\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_path, device_map = {\"\": torch.device(f\"cuda:{local_rank}\")}, \n",
    "#                                                               torch_dtype = torch.float16, \n",
    "                                                              trust_remote_code=True)\n",
    " \n",
    "\n",
    "    # TODO: Create a LoraConfig with the parameters: \n",
    "    # r=4, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=['lm_head.linear', 'transformer.embd.wte'].\n",
    "    # We will then use the config to initialize a LoraModelForCasualLM with the loaded model.\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    lora_config = LoraConfig(r = 8, lora_alpha = 16, lora_dropout = 0.05, bias = \"none\", task_type = \"CAUSAL_LM\", target_modules=['lm_head.linear', 'transformer.embd.wte']) \n",
    " \n",
    "\n",
    "    # TODO: Create LoRA model\n",
    "\n",
    "    model = LoraModelForCasualLM(model, lora_config)  # Apply current model to Lora Model\n",
    "#     model = get_peft_model(model, lora_config) # not sure\n",
    "    if _is_master_process():\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    OUTPUT_DIR = \"checkpoints/\"\n",
    "\n",
    "    backend = \"nccl\"\n",
    "    model_path = 'TheBloke/phi-2-GPTQ'\n",
    "    if os.environ.get(\"DEBUG\"):\n",
    "        data_path = \"test_data.json\"\n",
    "    else:\n",
    "        data_path = 'alpaca_data.json'\n",
    "\n",
    "    size_valid_set = 0.15\n",
    "    max_length = 128\n",
    "    num_epochs = 3\n",
    "    batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "    lr_scheduler_type = 'cosine'\n",
    "    num_warmup_steps = 100\n",
    "    weight_decay = 0.06\n",
    "\n",
    "    seed = 0\n",
    "    log_freq = 1\n",
    "    eval_freq = 150\n",
    "\n",
    "    distributed_strategy = \"ddp\" if os.environ.get(\"ON_DDP\") else \"no\"\n",
    "\n",
    "    if distributed_strategy == \"ddp\":\n",
    "\n",
    "        # TODO: Initialize the process group for distributed data parallelism with nccl backend.\n",
    "        # After that, you should set the 'local_rank' from the environment variable 'LOCAL_RANK'.\n",
    "\n",
    "        # Initialize the process group\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        torch.distributed.init_process_group(backend = backend)  ### YOUR CODE HERE ###\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\"))\n",
    "        pass\n",
    "    else:\n",
    "        os.environ['RANK'] = '0'\n",
    "        local_rank = 0\n",
    "\n",
    "    # Prepare model\n",
    "    model = load_pretrained_model(local_rank, model_path=model_path)\n",
    "    \n",
    "    # Get tokenizer\n",
    "    tokenizer = load_tokenizer_from_pretrained_model(model_path=model_path)\n",
    "\n",
    "    # prepare trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        num_epochs=num_epochs,\n",
    "        max_length=max_length,\n",
    "        batch_size=batch_size,\n",
    "        gpu_id=local_rank,\n",
    "        \n",
    "        mixed_precision_dtype=torch.float16 if os.environ.get(\"ON_MP\") else None,\n",
    "        \n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        is_ddp_training=True if distributed_strategy == \"ddp\" else False,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    # set ddp for wraping model\n",
    "    # execute trainer\n",
    "    trainer.run(\n",
    "        data_path=data_path,\n",
    "        size_valid_set=size_valid_set,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    if distributed_strategy == \"ddp\":\n",
    "        destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
