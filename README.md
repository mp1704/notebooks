# Seminar

## References
- [Transformer Math 101](https://blog.eleuther.ai/transformer-math/)
- [Everything about Distributed Training and Efficient Finetuning](https://sumanthrh.com/post/distributed-and-efficient-finetuning/#fully-sharded-data-parallel)
- [Training on one GPU](https://huggingface.co/docs/transformers/v4.29.0/perf_train_gpu_one#:~:text=A%20standard%20AdamW%20uses%208,all%20optimizer%20states%20are%20quantized.)